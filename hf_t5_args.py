from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union


@dataclass
class ScriptArguments:
    model_name_or_path: str = field(
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )

    sparql_dataset_name: str = field(default=False, metadata={"help": "Name of dataset to tune on - rubq vs salute"})
    language: str = field(default='ru', metadata={"help": "for RUBQ language, Salute in Russian"})

    path_to_training_file: str = field(default=None,
                                       metadata={"help": "Data path to training SPARQL dataset file"})
    path_to_testing_file: str = field(default=None,
                                      metadata={"help": "Data path to testing SQL dataset file"})
    path_to_predicate_description: str = field(default=None,
                                               metadata={"help": "Desription of predicates"})
    max_seq_length: int = field(
        default=512,
        metadata={
            "help": "The maximum total input sequence length after tokenization. Sequences longer "
                    "than this will be truncated, sequences shorter will be padded."
        },
    )
    max_output_length: int = field(
        default=512,
        metadata={
            "help": "The maximum length of an answer that can be generated. This is needed because the start "
                    "and end predictions are not conditioned on one another."
        },
    )
    max_new_tokens: int = field(
        default=512,
        metadata={
            "help": "The maximum length of an answer that can be generated. This is needed because the start "
                    "and end predictions are not conditioned on one another."
        },
    )

    num_beams: Optional[int] = field(
        default=20,
        metadata={
            "help": "Number of beams to use for evaluation. This argument will be passed to ``model.generate``, "
                    "which is used during ``evaluate`` and ``predict``."
        },
    )

    try_one_batch: bool = field(default=False, metadata={"help": "Try training with one batch"})

    phase: str = field(
        default="original",
        metadata={
            "help": 'Phase of training - "pretrain" or "finetune" or "original"'
        },
    )
#
#
#
# @dataclass
# class ModelArguments:
#     """
#     Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
#     """
#
#     model_name_or_path: str = field(
#         metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
#     )
#     config_name: Optional[str] = field(
#         default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
#     )
#     tokenizer_name: Optional[str] = field(
#         default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
#     )
#     cache_dir: Optional[str] = field(
#         default=None,
#         metadata={"help": "Path to directory to store the pretrained models downloaded from huggingface.co"},
#     )
#     use_fast_tokenizer: bool = field(
#         default=True,
#         metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
#     )


# @dataclass
# class DataTrainingArguments:
#     """
#     Arguments pertaining to what data we are going to input our model for training and eval.
#     """
#     is_tuning: bool = field(
#         default=False,
#         metadata={
#             "help": "Whether we are tunning hyperparameters. "
#                     "If True, will automatically split the training set into validation set "
#         },
#     )
#
#     dataset_name: Optional[str] = field(
#         default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
#     )
#     dataset_config_name: Optional[str] = field(
#         default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
#     )
#
#     train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
#     validation_file: Optional[str] = field(
#         default=None,
#         metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
#     )
#     test_file: Optional[str] = field(
#         default=None,
#         metadata={"help": "An optional input test data file to evaluate the perplexity on (a text file)."},
#     )
#
#     max_seq_length: int = field(
#         default=512,
#         metadata={
#             "help": "The maximum total input sequence length after tokenization. Sequences longer "
#                     "than this will be truncated, sequences shorter will be padded."
#         },
#     )
#     max_output_length: int = field(
#         default=512,
#         metadata={
#             "help": "The maximum length of an answer that can be generated. This is needed because the start "
#                     "and end predictions are not conditioned on one another."
#         },
#     )
#
#     pad_to_max_length: bool = field(
#         default=True,
#         metadata={
#             "help": "Whether to pad all samples to `max_seq_length`. "
#                     "If False, will pad the samples dynamically when batching to the maximum length in the batch (which can "
#                     "be faster on GPU but will be slower on TPU)."
#         },
#     )
#
#     n_best_size: int = field(
#         default=20,
#         metadata={"help": "The total number of n-best predictions to generate when looking for an answer."},
#     )
#     num_beams: Optional[int] = field(
#         default=20,
#         metadata={
#             "help": "Number of beams to use for evaluation. This argument will be passed to ``model.generate``, "
#                     "which is used during ``evaluate`` and ``predict``."
#         },
#     )
#     ignore_pad_token_for_loss: bool = field(
#         default=True,
#         metadata={
#             "help": "Whether to ignore the tokens corresponding to padded labels in the loss computation or not."
#         },
#
#     )
#     cuda_visible_device: str = field(
#         default=False,
#     )
#     try_one_batch: bool = field(
#         default=False,
#         metadata={
#             "help": "Whether to ignore the tokens corresponding to padded labels in the loss computation or not."
#         }
#     )
#
#
#     def __post_init__(self):
#         if (
#                 self.dataset_name is None
#                 and self.train_file is None
#                 and self.validation_file is None
#                 and self.test_file is None
#         ):
#             raise ValueError("Need either a dataset name or a training/validation file/test_file.")
#         else:
#             if self.train_file is not None:
#                 extension = self.train_file.split(".")[-1]
#                 assert extension in ["csv", "tsv"], "`train_file` should be a csv or tsv file."
#             if self.validation_file is not None:
#                 extension = self.validation_file.split(".")[-1]
#                 assert extension in ["csv", "tsv"], "`validation_file` should be a csv or tsv file."
#             if self.test_file is not None:
#                 extension = self.test_file.split(".")[-1]
#                 assert extension in ["csv", "tsv"], "`test_file` should be a csv or tsv file."
#
#
# @dataclass
# class ExperimentArgs:
#     cp_mode: bool = field(
#         default=False,
#         metadata={
#             "help": 'Compgen training'
#         },
#     ),
#     phase: str = field(
#         default="pretrain",
#         metadata={
#             "help": 'Phase of training - "pretrain" or "finetune" or "original"'
#         },
#     ),
#     pretrain_ratio: float = field(
#         default=0.0,
#         metadata={
#             "help": 'Ratio of training steps for pre-training'
#         },
#     )
